---
title: "Data Science Homework 6"
author: "Thirsten Stockton"
date: "2022-12-03"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(modelr)
library(purrr)
library(glmnet)
```

# Problem 1

**Code to bring in data**

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

***Plotting distribution of bootstapped r-sqaured estimates***
```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```


***95% confidence interval for r-squared estimate***

```{r}
rsq_bootstrap =
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) 

quantile(rsq_bootstrap$r.squared, c(0.025, 0.975))
```

The distribution of r-squared estimates shows a slight left skew and is rather narrow. The estimated 95% confidence interval is (0.8937494, 0.9274768), though due to the skewed nature of the data, this may not be the best estimate. 

***Plotting distribution of $\log(\beta_0 * \beta1)$***

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

***95% confidence interval for $\log(\beta_0 * \beta1)$ estimate***
```{r}

log_betas_bootstrap =
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) 


quantile(log_betas_bootstrap$log_b0b1, c(0.025, 0.975))
```

Similar to the r-squared plot, this plot shows a slight left skew. The 95% confidence interval for this estimate is (1.962837, 2.059418), but like the first estimate, the 95% CI may not be reliable due to skewness.


### Problem 2

### Reading in and cleaning data
```{r}
homocide_df = 
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>%
  unite(city_state,c(city, state), sep = ", ", remove=FALSE) %>%
  filter(city_state != "Dallas, Tx", city_state != "Phoenix, AZ", city_state != "Kansas City, MO", city_state != "Tulsa, AL")  %>%
  filter(victim_race %in% c("White", "Black")) %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  mutate(crime = case_when(
    disposition %in% c("Closed without arrest", "Open/No arrest") ~ "1",
    disposition %in% c("Closed by arrest") ~ "0")) %>%
  mutate(crime = as.numeric(crime)) 

```


### Logistic regression model for Baltimore, MD.

```{r}

bmd_log= 
  homocide_df %>%
  filter(city_state == "Baltimore, MD") %>%
  glm(crime ~ victim_age + victim_race + victim_sex, family = binomial(), data = .)


bmd_log %>% 
  broom::tidy() %>%
  mutate(OR = exp(estimate)) %>%
  mutate(LCI = exp(estimate-(1.96*std.error))) %>%
  mutate(UCI = exp(estimate+(1.96*std.error))) %>%
  select(term, log_OR = estimate, OR, LCI, UCI) %>%
  filter(term == "victim_sexMale")
  

```

### Mapping to every city state

```{r}
broom_tidy_function = function(data) {
  data %>%
    broom::tidy() %>%
    mutate(OR = exp(estimate)) %>%
    mutate(LCI = exp(estimate-(1.96*std.error))) %>%
    mutate(UCI = exp(estimate+(1.96*std.error))) %>%
    select(term, log_OR = estimate, OR, LCI, UCI) %>%
    filter(term == "victim_sexMale")
}

city_state_log =
  homocide_df %>%
  nest(data = -city_state) %>%
  mutate(log_models = map(data, ~glm(crime ~ victim_age + victim_race + victim_sex, family = binomial(), data = .))) %>%
  mutate(results = (map(log_models, broom_tidy_function ))) %>%
  select(city_state, results) %>%
    unnest(cols = results)

city_state_log

```

### Plot of OR's and their 95% CI by city.

```{r}

OR_plot =
city_state_log %>%
  ggplot(., aes (y = reorder(city_state, -OR), x= OR)) +
      geom_point(shape = 19, size=1) +
      geom_errorbarh(aes(xmin = LCI, xmax = UCI, height = 0.25)) +
      geom_vline(xintercept = 1, color = "purple", alpha = 0.5) 

OR_plot
```

This plot shows the odds ratios comparing odds of a male victim having their homicide go unsolved versus a female victim, by city. Looking at this plot, we can see that most of the odds ratio were not significant. Male victims tended to have higher odds of having their homicide go unsolved across cities. New York City boasted the largest odds ratio, but also had a fairly large confidence interval. 

### Problem 3

### **Birthweight regression model using LASSO and OLS**

### Bringing in and cleaning birthweight dataset

```{r}

bwt_df = 
  read_csv("./birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4", "other" = "8"),
    malform = as.logical(malform),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4")) %>%
    sample_n(200)

bwt_df

```

### Finding optimal lambda
```{r}
y = bwt_df$bwt
x = model.matrix(bwt ~ ., bwt_df)[,-1]

val_model =
  cv.glmnet(x, y)

opt_lambda = 
  val_model$lambda.min

opt_lambda

```

### Fitting lasso regression model

```{r}

lasso_bwt =
  glmnet(x, y)

