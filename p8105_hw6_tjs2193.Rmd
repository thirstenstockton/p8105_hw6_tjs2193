---
title: "Data Science Homework 6"
author: "Thirsten Stockton"
date: "2022-12-03"
output: github_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(modelr)
library(purrr)
library(glmnet)
set.seed(123)
options(tibble.print_min = 10)
```

# Problem 1

**Code to bring in data**

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

***Plotting distribution of bootstapped r-sqaured estimates***
```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```


***95% confidence interval for r-squared estimate***

```{r}
rsq_bootstrap =
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) 

quantile(rsq_bootstrap$r.squared, c(0.025, 0.975))
```

The distribution of r-squared estimates shows a slight left skew and is rather narrow. The estimated 95% confidence interval is (0.8937494, 0.9274768), though due to the skewed nature of the data, this may not be the best estimate. 

***Plotting distribution of $\log(\beta_0 * \beta1)$***

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

***95% confidence interval for $\log(\beta_0 * \beta1)$ estimate***
```{r}

log_betas_bootstrap =
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) 


quantile(log_betas_bootstrap$log_b0b1, c(0.025, 0.975))
```

Similar to the r-squared plot, this plot shows a slight left skew. The 95% confidence interval for this estimate is (1.962837, 2.059418), but like the first estimate, the 95% CI may not be reliable due to skewness.


### Problem 2

### Reading in and cleaning data
```{r}
homocide_df = 
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>%
  unite(city_state,c(city, state), sep = ", ", remove=FALSE) %>%
  filter(city_state != "Dallas, Tx", city_state != "Phoenix, AZ", city_state != "Kansas City, MO", city_state != "Tulsa, AL")  %>%
  filter(victim_race %in% c("White", "Black")) %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  mutate(crime = case_when(
    disposition %in% c("Closed without arrest", "Open/No arrest") ~ "1",
    disposition %in% c("Closed by arrest") ~ "0")) %>%
  mutate(crime = as.numeric(crime)) 

```


### Logistic regression model for Baltimore, MD.

```{r}

bmd_log= 
  homocide_df %>%
  filter(city_state == "Baltimore, MD") %>%
  glm(crime ~ victim_age + victim_race + victim_sex, family = binomial(), data = .)


bmd_log %>% 
  broom::tidy() %>%
  mutate(OR = exp(estimate)) %>%
  mutate(LCI = exp(estimate-(1.96*std.error))) %>%
  mutate(UCI = exp(estimate+(1.96*std.error))) %>%
  select(term, log_OR = estimate, OR, LCI, UCI) %>%
  filter(term == "victim_sexMale")
  

```

### Mapping to every city state

```{r}
broom_tidy_function = function(data) {
  data %>%
    broom::tidy() %>%
    mutate(OR = exp(estimate)) %>%
    mutate(LCI = exp(estimate-(1.96*std.error))) %>%
    mutate(UCI = exp(estimate+(1.96*std.error))) %>%
    select(term, log_OR = estimate, OR, LCI, UCI) %>%
    filter(term == "victim_sexMale")
}

city_state_log =
  homocide_df %>%
  nest(data = -city_state) %>%
  mutate(log_models = map(data, ~glm(crime ~ victim_age + victim_race + victim_sex, family = binomial(), data = .))) %>%
  mutate(results = (map(log_models, broom_tidy_function ))) %>%
  select(city_state, results) %>%
    unnest(cols = results)

city_state_log

```

### Plot of OR's and their 95% CI by city.

```{r}

OR_plot =
city_state_log %>%
  ggplot(., aes (y = reorder(city_state, -OR), x= OR)) +
      geom_point(shape = 19, size=1) +
      geom_errorbarh(aes(xmin = LCI, xmax = UCI, height = 0.25)) +
      geom_vline(xintercept = 1, color = "purple", alpha = 0.5) 

OR_plot
```

This plot shows the odds ratios comparing odds of a male victim having their homicide go unsolved versus a female victim, by city. Looking at this plot, we can see that most of the odds ratio were not significant. Male victims tended to have higher odds of having their homicide go unsolved across cities. New York City boasted the largest odds ratio, but also had a fairly large confidence interval. 

### Problem 3

### **Birthweight regression model using LASSO**

### Bringing in and cleaning birthweight dataset for LASSO Regression.

General cleaning

```{r}

bwt_df = 
  read_csv("./birthweight.csv") %>% 
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4", "other" = "8"),
    malform = as.logical(malform),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4")) 

```

Adding column ID in order to split data into training and test sets.
```{r}
bwt_df =
  tibble::rowid_to_column(bwt_df, "id")

bwt_df

```

Splitting data into training and test sets.
```{r}
bwt_df_train =
  sample_n(bwt_df, 200)



bwt_df_test = 
  anti_join(bwt_df, bwt_df_train, by = "id")

bwt_df_test =
 bwt_df_test  %>%
  select(-id)


bwt_df_train =
  bwt_df_train %>%
    select(-id)

bwt_df_train

bwt_df_test

```

### Finding optimal lambda and creating test objects for model building and validation
```{r}
y = bwt_df_train$bwt
x1 = model.matrix(bwt ~ ., bwt_df_train)[,-1]

y_test = bwt_df_test$bwt
x1_test = model.matrix(bwt ~ ., bwt_df_test)[,-1]

val_model =
  cv.glmnet(x1, y)

opt_lambda = 
  val_model$lambda.min

opt_lambda

```

### Fitting lasso regression model

```{r}

lasso_bwt_train =
  glmnet(x1, y, lambda = opt_lambda)

```

### Results of our model

```{r}

lasso_bwt_train %>%
  broom::tidy()

```

### Plot of predictions versus residuals

```{r}
predicted =
  predict(lasso_bwt_train, s = opt_lambda, newx = x1_test) 

residual =
  predicted - y_test %>%
  as.tibble()

fittedvalues =
  bwt_df_test$bwt

predicted =
  predicted %>%
  as.tibble

plot_df =
  cbind(fittedvalues, predicted, residual) %>%
  as.tibble()


plot_df

plot =
  plot_df %>%
  ggplot(aes(x = fittedvalues, y = value)) + geom_point(size = 0.25, alpha = 0.8)

plot

```

To build this model predicting birth weight, I split data into training and test sets then fit a LASSO regression model to the training data. After that, I used the model to predict values for individual in the test set and plotted their residuals. The residuals show a fairly symmetrical pattern, clustered around zero, indicating that this is a useful model. There are a few outliers noted, such as the the one around ~3500 g bwt.